{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535813aa-cc62-4377-9aaf-b2c7e08346a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16842e59-019c-433d-88ab-ad065cd72e6c",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Here, the aim is to make the 3 DataLoaders: Train, Val, Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2962f4af-fa79-4635-a707-44ef3de08510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"../../data/friends.train.scene_delim.conll\"\n",
    "test_path = \"../../data/friends.test.scene_delim.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c309ed-1e68-4b61-ae9b-28d3d38ad97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_conllu(in_p):\n",
    "    with open(in_p, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\"\\n\")\n",
    "    # iterate through all lines and get fields\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    for line in data:\n",
    "        if line.startswith(\"#\"):  # skip comments\n",
    "            continue\n",
    "        if line.strip() == \"\":  # end of sentence\n",
    "            sentences.append(tokens)\n",
    "            tokens = []\n",
    "            continue\n",
    "        line = line.replace(\"/  +/g\", ' ')\n",
    "        fields = line.split()\n",
    "        token = {\n",
    "            # \"doc id\": fields[0],\n",
    "            # \"scene id\": fields[1],\n",
    "            # \"token id\": fields[2],\n",
    "            \"form\": fields[3].lower(),\n",
    "            \"speaker\": fields[9],\n",
    "            \"referenced\": fields[11]\n",
    "        }\n",
    "        tokens.append(token)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8d8254-e0c8-4ee8-8f40-533c517a3e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_scenes(list_scene):\n",
    "    # combine every 24 scenes into single long scene\n",
    "    l = len(list_scene)\n",
    "    fin = []\n",
    "    for i in range(0, l, 24):\n",
    "        temp = []\n",
    "        if i+24<=l:\n",
    "            part = list_scene[i:i+24]\n",
    "        else:\n",
    "            part = list_scene[i:]\n",
    "        for j in part:\n",
    "            temp += j\n",
    "        fin.append(temp)\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3978bb55-5823-4844-b527-88d037377f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_cont_ref(list_scene):\n",
    "    # FIX CONTINUOUS REFERENCES\n",
    "    ## if consecutive words reference the same entity then \n",
    "    ## the last words references it.\n",
    "    for scene in list_scene:\n",
    "        for token in scene:\n",
    "            ref = token[\"referenced\"]\n",
    "            if \"(\" in ref:\n",
    "                if \")\" not in ref:\n",
    "                    token[\"referenced\"] = \"-\"\n",
    "            ref = token[\"referenced\"]\n",
    "            # convert reference num to int\n",
    "            if ref!=\"-\":\n",
    "                num = \"\"\n",
    "                for char in ref:\n",
    "                    if char.isalnum():\n",
    "                        num += char\n",
    "                num = int(num)\n",
    "            else:\n",
    "                num = -1\n",
    "            token[\"referenced\"] = num\n",
    "    return list_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c36af4-96bc-46ad-b08d-c088f6161e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = fix_cont_ref(combine_scenes(read_conllu(train_path)))\n",
    "test = fix_cont_ref(combine_scenes(read_conllu(test_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31255878-2083-4d4e-a4eb-cd28cecd0380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "           \"idx2char\":{},\n",
    "           \"char2idx\":{}\n",
    "          }\n",
    "with open(\"../../data/friends_entity_map.txt\", \"r\") as f:\n",
    "    idx_char = f.read()\n",
    "idx_char = idx_char.split(\"\\n\")[:-1]\n",
    "for i in idx_char:\n",
    "    i_c = i.split(\"\\t\")\n",
    "    z = \"_\".join(i_c[1].split(\" \"))\n",
    "    mapping[\"idx2char\"][i_c[0]] = z\n",
    "    mapping[\"char2idx\"][z] = int(i_c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cf56b-adb8-4325-ac8a-5801082edf65",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dealing with Referenced\n",
    "## All charecters with freq < 3 are set to \"\\<OTH>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b3bc2e4-6f9d-4430-a3d1-f909673ba312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for scene in train:\n",
    "    for token in scene:\n",
    "        if token[\"referenced\"]==-1:\n",
    "            token[\"referenced\"] = \"-\"\n",
    "        else:\n",
    "            token[\"referenced\"] = mapping[\"idx2char\"][str(token[\"referenced\"])]\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        if token[\"referenced\"]==-1:\n",
    "            token[\"referenced\"] = \"-\"\n",
    "        else:\n",
    "            token[\"referenced\"] = mapping[\"idx2char\"][str(token[\"referenced\"])]\n",
    "count = {}\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        ref = token[\"referenced\"]\n",
    "        if ref in count:\n",
    "            count[ref] += 1\n",
    "        else:\n",
    "            count[ref] = 1\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        ref = token[\"referenced\"]\n",
    "        if ref in count:\n",
    "            count[ref] += 1\n",
    "        else:\n",
    "            count[ref] = 1\n",
    "# All low freq go to \"<OTH>\"\n",
    "to_oth = {}\n",
    "for char in count:\n",
    "    if count[char]<=20:              # change from orig paper cut off for extra is 20 instead of 3\n",
    "        to_oth[char] = \"<OTH>\"\n",
    "    else:\n",
    "        to_oth[char] = char\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        token[\"referenced\"] = to_oth[token[\"referenced\"]]\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        token[\"referenced\"] = to_oth[token[\"referenced\"]]\n",
    "# Create new mapping char -> idx\n",
    "char2idx = {}\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        char = token[\"referenced\"]\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        char = token[\"referenced\"]\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        token[\"referenced\"] = char2idx[token[\"referenced\"]]\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        token[\"referenced\"] = char2idx[token[\"referenced\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797bea80-db1f-4adb-ac07-05a378398c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f936aac-1055-48f0-b562-bce6815e2db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f271434-8de5-4238-9010-e14171d17b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../../output/char2idx.json\", \"w\") as f:\n",
    "    json.dump(char2idx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af547c85-1eac-4839-97a9-4dc1170f7fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dealing with Speaker\n",
    "### Replace with vocab from entity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97108fc-4468-4ded-8072-3044178ffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Other term to char2idx\n",
    "fin = {}\n",
    "for i in mapping[\"char2idx\"]:\n",
    "    if i in char2idx:\n",
    "        fin[i] = char2idx[i]\n",
    "    else:\n",
    "        fin[i] = char2idx[\"<OTH>\"]\n",
    "mapping[\"char2idx\"] = fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134d8b62-2b52-4ea1-bf8d-5462614379d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Done---------------\n"
     ]
    }
   ],
   "source": [
    "for scene in train:\n",
    "    for token in scene:\n",
    "        sp = token[\"speaker\"]\n",
    "        if \",\" in sp:\n",
    "            chars = \"\".join(sp.split(\"_\"))\n",
    "            chars = chars.split(\",\")\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                if i==\"Chandler\":\n",
    "                    i = \"Chandler_Bing\"\n",
    "                if i==\"Ross\":\n",
    "                    i = \"Ross_Geller\"\n",
    "                if i==\"Joey\":\n",
    "                    i = \"Joey_Tribbiani\"\n",
    "                if i==\"Phoebe\":\n",
    "                    i = \"Phoebe_Buffay\"\n",
    "                if i==\"Monica\":\n",
    "                    i = \"Monica_Geller\"\n",
    "                if i==\"Rachel\":\n",
    "                    i = \"Rachel_Green\"\n",
    "                if i==\"Carol\":\n",
    "                    i = \"Carol_Willick\"\n",
    "                if i==\"Susan\":\n",
    "                    i = \"Susan_Bunch\"\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Boys\":\n",
    "            chars = ['Chandler_Bing', 'Joey_Tribbiani', 'Ross_Geller']\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Gang\" or sp==\"Everyone\":\n",
    "            chars = ['Chandler_Bing', 'Joey_Tribbiani', 'Ross_Geller', 'Phoebe_Buffay','Monica_Geller', 'Rachel_Green']\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Mr._Green\":\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][\"Mr._Greene\"]\n",
    "        elif sp==\"Joey's_Co-Star\":\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][\"Joey's_Co-star\"]\n",
    "        else:\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][token[\"speaker\"]]\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        sp = token[\"speaker\"]\n",
    "        if \",\" in sp:\n",
    "            chars = \"\".join(sp.split(\"_\"))\n",
    "            chars = chars.split(\",\")\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                if i==\"Chandler\":\n",
    "                    i = \"Chandler_Bing\"\n",
    "                if i==\"Ross\":\n",
    "                    i = \"Ross_Geller\"\n",
    "                if i==\"Joey\":\n",
    "                    i = \"Joey_Tribbiani\"\n",
    "                if i==\"Phoebe\":\n",
    "                    i = \"Phoebe_Buffay\"\n",
    "                if i==\"Monica\":\n",
    "                    i = \"Monica_Geller\"\n",
    "                if i==\"Rachel\":\n",
    "                    i = \"Rachel_Green\"\n",
    "                if i==\"Carol\":\n",
    "                    i = \"Carol_Willick\"\n",
    "                if i==\"Susan\":\n",
    "                    i = \"Susan_Bunch\"\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Boys\":\n",
    "            chars = ['Chandler_Bing', 'Joey_Tribbiani', 'Ross_Geller']\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Gang\" or sp==\"Everyone\":\n",
    "            chars = ['Chandler_Bing', 'Joey_Tribbiani', 'Ross_Geller', 'Phoebe_Buffay','Monica_Geller', 'Rachel_Green']\n",
    "            t = []\n",
    "            for i in chars:\n",
    "                t.append(mapping[\"char2idx\"][i])\n",
    "            token[\"speaker\"] = t\n",
    "        elif sp==\"Mr._Green\":\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][\"Mr._Greene\"]\n",
    "        elif sp==\"Joey's_Co-Star\":\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][\"Joey's_Co-star\"]\n",
    "        else:\n",
    "            token[\"speaker\"] = mapping[\"char2idx\"][token[\"speaker\"]]\n",
    "print(\"--------Done---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228b1743-a169-4881-a0b2-2e5d417f93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_dim = len(char2idx)\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        t = np.zeros(ohe_dim)\n",
    "        chars = token[\"speaker\"]\n",
    "        if isinstance(chars, list):\n",
    "            for i in chars:\n",
    "                t[i] += 1\n",
    "        else:\n",
    "            t[chars] += 1\n",
    "        token[\"speaker\"] = t\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        t = np.zeros(ohe_dim)\n",
    "        chars = token[\"speaker\"]\n",
    "        if isinstance(chars, list):\n",
    "            for i in chars:\n",
    "                t[i] += 1\n",
    "        else:\n",
    "            t[chars] += 1\n",
    "        token[\"speaker\"] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca008940-868a-419b-b9f3-3769b64bb39c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dealing with Forms\n",
    "### convert all \"forms\" to \"vec\" using Google's News 300 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64ca976-d3aa-429b-9dcb-df7e372fd99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forms2vec = {}\n",
    "for scene in train:\n",
    "    for token in scene:\n",
    "        forms2vec[token[\"form\"]] = 1\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        forms2vec[token[\"form\"]] = 1\n",
    "# load model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../../huggin_face/GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
    "# generate embedding\n",
    "for form in forms2vec:\n",
    "    if form in model:\n",
    "        forms2vec[form] = model[form]\n",
    "    else:\n",
    "        forms2vec[form] = [0]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de1e38d-2539-4ce4-b616-885d12456b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for scene in train:\n",
    "    for token in scene:\n",
    "        token[\"form\"] = forms2vec[form]\n",
    "for scene in test:\n",
    "    for token in scene:\n",
    "        token[\"form\"] = forms2vec[form]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f814e1-6de3-4649-a6d1-1f4421492c14",
   "metadata": {},
   "source": [
    "# Generate DataLoaders and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b24a594-fcea-436e-8b36-d202fea5b7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train into train+val\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc9980a-0391-446b-95aa-9222446bd85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = train[500:]\n",
    "train = train[:500]\n",
    "test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adb7c97e-c4bb-4acf-8e37-0f8c6036b4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_matrices(dt_in):\n",
    "    fin = []\n",
    "    for scene in dt_in:\n",
    "        X = []\n",
    "        y = []\n",
    "        for token in scene:\n",
    "            t1 = [token[\"form\"], token[\"speaker\"]]\n",
    "            t2 = [token[\"referenced\"]]\n",
    "            X.append(t1)\n",
    "            y.append(t2)\n",
    "        fin.append([X,y])\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76e5e65c-f56f-4014-8ba2-fa6d561fca5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train= gen_matrices(train)\n",
    "val = gen_matrices(val)\n",
    "test = gen_matrices(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67277089-d3e2-462c-adf8-0586e2bec6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train, batch_size=1, shuffle=True)\n",
    "val_dl = DataLoader(val, batch_size=1, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a3564e-d9ce-4cd3-b5c1-88011a3e2c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save all DataLoaders locally\n",
    "import pickle\n",
    "\n",
    "with open('../../output/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dl, f)\n",
    "with open('../../output/val.pkl', 'wb') as f:\n",
    "    pickle.dump(val_dl, f)\n",
    "with open('../../output/test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b1c98-dd64-4399-bafa-67a80c6472f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
